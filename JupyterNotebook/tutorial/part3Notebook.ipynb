{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e6fdced-c355-493b-8550-de9c7292db7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "model = Word2Vec.load(\"300features_40minwords_10context\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d385de90-e1bb-4b73-a040-9d32d705b7f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(model.wv.vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a349310b-4fdc-43a3-ac6a-64a9d6e26269",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16490, 300)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 查看所有词向量组成的矩阵的形状\n",
    "model.wv.vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7896ce05-5ede-4254-8625-8217a0d287b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.70418285e-02,  4.17075828e-02,  4.68027405e-02,  5.77561073e-02,\n",
       "       -4.90124077e-02, -9.11914781e-02,  1.25736268e-02,  2.53656775e-01,\n",
       "        2.55882926e-02, -9.93438065e-02, -3.88848074e-02,  5.98192168e-03,\n",
       "        5.11968732e-02,  8.12970102e-02, -4.02569361e-02, -9.01165977e-03,\n",
       "       -4.89391647e-02, -1.06224619e-01, -1.51742483e-03,  4.80879843e-02,\n",
       "       -5.73582249e-03,  3.22901085e-02, -3.04702036e-02,  1.21747874e-01,\n",
       "        1.15162851e-02, -1.89318955e-02,  3.02785505e-02, -1.88510306e-02,\n",
       "       -1.93555709e-02,  1.73930470e-02,  8.91037136e-02, -3.36124338e-02,\n",
       "        1.24498142e-03,  2.34751496e-03, -7.73967197e-03,  3.07026040e-02,\n",
       "        3.07262540e-02, -1.02033511e-01, -1.09931326e-03,  9.13682282e-02,\n",
       "       -4.12567668e-02,  3.09050386e-03, -1.20065305e-02, -2.85921004e-02,\n",
       "       -2.22854987e-02,  5.53633608e-02,  6.29955754e-02,  4.77065556e-02,\n",
       "        8.30905791e-03,  4.66902144e-02,  1.02785520e-01,  7.69525170e-02,\n",
       "        1.48280161e-02, -2.04587560e-02, -2.18986464e-03,  1.26254130e-02,\n",
       "       -2.68392600e-02, -1.49295675e-02, -1.38285041e-01, -8.38618800e-02,\n",
       "       -1.55051082e-01,  1.96400844e-03,  1.99162103e-02, -1.76339392e-02,\n",
       "        1.52206013e-03, -2.74715666e-02, -7.13702068e-02, -3.82266864e-02,\n",
       "       -6.27597934e-03, -6.98105767e-02,  2.64001191e-02, -1.54403783e-02,\n",
       "        1.07084513e-01,  9.90289915e-03, -3.28061804e-02, -8.82632751e-03,\n",
       "       -1.61318555e-02,  4.45394516e-02, -6.70952275e-02,  3.11420187e-02,\n",
       "        1.50552085e-02,  4.99025322e-02, -1.37721719e-02,  4.53230329e-02,\n",
       "       -4.71027680e-02, -3.01369629e-03,  2.28678510e-02,  4.58697379e-02,\n",
       "        3.77932042e-02, -7.52767222e-03, -6.17732890e-02,  7.88925681e-03,\n",
       "       -4.41310182e-02,  5.36591932e-02,  4.69872057e-02, -1.11710355e-02,\n",
       "        7.65355229e-02, -1.63626261e-02, -1.08818561e-01,  3.05730328e-02,\n",
       "        4.33612280e-02,  4.86018993e-02,  1.27269244e-02, -9.57477242e-02,\n",
       "        1.85736716e-02, -2.71264296e-02, -6.65341616e-02, -5.02695562e-03,\n",
       "        9.04954672e-02,  2.59044790e-03, -7.44170472e-02, -1.04901111e-02,\n",
       "        7.38357231e-02,  2.89660804e-02,  6.21622242e-02,  8.80038962e-02,\n",
       "        1.10127553e-01,  2.77297162e-02,  3.56210656e-02, -7.10836649e-02,\n",
       "        5.67067116e-02,  2.56871153e-02,  4.42388393e-02, -2.66686056e-05,\n",
       "       -4.43877690e-02,  1.55971982e-02,  4.62084673e-02, -4.55241501e-02,\n",
       "        4.09619734e-02, -2.28808112e-02,  3.15967500e-02,  4.25419733e-02,\n",
       "       -2.22660825e-02, -1.24372169e-02,  2.51970422e-02, -4.28898558e-02,\n",
       "       -1.79354064e-02,  3.53298336e-02, -6.03421591e-02,  8.44309181e-02,\n",
       "       -6.61665425e-02, -1.03020594e-02, -4.91442382e-02,  2.92656384e-02,\n",
       "       -7.23880576e-03, -7.37397596e-02, -7.71360472e-02, -2.00487114e-02,\n",
       "        5.34291640e-02,  5.64166382e-02, -3.34761553e-02,  1.37939556e-02,\n",
       "       -6.12148456e-02,  2.01643887e-03,  4.54769172e-02, -1.15679363e-02,\n",
       "       -8.10043663e-02, -4.13050018e-02, -3.94560620e-02,  1.34112820e-01,\n",
       "        1.43590877e-02, -6.11354671e-02, -2.83829682e-02,  6.72790185e-02,\n",
       "       -6.92004934e-02, -3.71951573e-02,  1.10569350e-01, -4.83626388e-02,\n",
       "       -3.62411737e-02, -3.51215489e-02,  8.08097143e-03,  1.44472085e-02,\n",
       "       -3.52260284e-02, -2.29860134e-02, -9.11664888e-02, -3.63596901e-02,\n",
       "        4.38784398e-02,  3.36347483e-02, -3.84763107e-02,  3.04275770e-02,\n",
       "        4.93894070e-02,  9.03367922e-02,  3.27038728e-02, -3.42693739e-02,\n",
       "        2.95867734e-02,  1.49664469e-02,  5.06730634e-04,  5.57104163e-02,\n",
       "        5.16457371e-02, -2.37483755e-02, -5.27327359e-02, -4.51146401e-02,\n",
       "       -4.15193178e-02,  2.90502049e-02,  2.53646038e-02, -1.67921800e-02,\n",
       "        1.35078616e-02, -9.09739882e-02,  5.42171337e-02, -5.33386320e-02,\n",
       "       -6.19115978e-02,  1.29552215e-01,  7.04893917e-02, -3.84167358e-02,\n",
       "       -8.44027251e-02,  3.37842479e-02,  2.09852140e-02,  9.25398245e-02,\n",
       "       -3.18566635e-02, -4.09171581e-02,  5.39813079e-02, -7.21935183e-02,\n",
       "        2.45472621e-02, -7.58749694e-02,  8.29440821e-03,  4.38865349e-02,\n",
       "       -2.94708405e-02, -1.94521517e-01,  4.67765704e-02, -3.82808894e-02,\n",
       "        9.43313166e-02,  9.97976437e-02, -3.30520608e-02, -3.67552228e-02,\n",
       "       -4.10101749e-02, -3.60015742e-02, -3.07331495e-02, -2.48591099e-02,\n",
       "       -6.14356063e-02,  1.52095337e-03,  4.21424732e-02, -2.44351719e-02,\n",
       "       -5.20514958e-02,  6.69689626e-02,  4.94178720e-02,  3.48995589e-02,\n",
       "       -4.01029550e-02,  2.05791723e-02,  4.81929351e-03, -6.32619411e-02,\n",
       "        8.35313872e-02, -1.06398776e-01, -1.38930073e-02, -4.86810282e-02,\n",
       "       -5.92579774e-04,  2.24003512e-02, -3.98566909e-02,  4.23725620e-02,\n",
       "        4.29614969e-02,  3.08401026e-02, -8.82156491e-02, -6.30137622e-02,\n",
       "       -2.09557684e-03, -3.70101966e-02,  4.36112434e-02, -1.07421994e-01,\n",
       "        1.25010520e-01,  4.02643494e-02, -5.99798784e-02, -7.23737404e-02,\n",
       "       -1.75292864e-02, -1.09130526e-02, -3.15660499e-02, -1.53352935e-02,\n",
       "       -3.68869230e-02, -3.84865105e-02,  5.07344604e-02,  5.07948697e-02,\n",
       "       -5.58685325e-02, -1.49018662e-02, -8.37744623e-02,  3.22891772e-02,\n",
       "       -3.60906199e-02, -5.39001748e-02,  1.19112805e-01, -3.93051654e-02,\n",
       "        6.87079579e-02, -1.01129776e-02, -1.13600351e-01, -7.72387662e-04,\n",
       "       -2.68963091e-02, -2.18190774e-02,  4.62400727e-03,  2.86485367e-02,\n",
       "       -1.22726196e-02, -2.06241570e-02, -8.31296726e-04,  1.04500487e-01,\n",
       "        4.81219776e-02,  4.27759588e-02, -1.34380758e-02,  9.20552313e-02,\n",
       "        2.22664654e-01,  6.64851591e-02, -1.13547128e-02,  1.15785256e-01,\n",
       "       -1.60378501e-01, -1.57122535e-03, -2.70612212e-03,  3.38710099e-02],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv[\"flower\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "840774bf-4bab-4f43-bfdd-92f189c3b440",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np  \n",
    "\n",
    "def makeFeatureVec(words, model, num_features):\n",
    "    # 该函数用于计算段落中所有词向量的平均值\n",
    "    #\n",
    "    # 预先初始化一个空的 numpy 数组（为了速度）\n",
    "    featureVec = np.zeros((num_features,), dtype=\"float32\")\n",
    "    #\n",
    "    nwords = 0.\n",
    "    # \n",
    "    # index2word 是一个包含模型词汇表中所有单词名称的列表。\n",
    "    # 为了速度，将其转换为集合（set）。\n",
    "    index2word_set = set(model.wv.index_to_key) # Note: .index2word is deprecated, .wv.index_to_key is the new syntax\n",
    "    #\n",
    "    # 遍历评论中的每个单词，如果它在模型的词汇表中，\n",
    "    # 就将其特征向量加到总和中。\n",
    "    for word in words:\n",
    "        if word in index2word_set: \n",
    "            nwords = nwords + 1.\n",
    "            featureVec = np.add(featureVec, model.wv[word]) # Note: model[word] is deprecated, model.wv[word] is the new syntax\n",
    "    # \n",
    "    # 将结果除以单词总数，得到平均值。\n",
    "    if nwords > 0:\n",
    "        featureVec = np.divide(featureVec, nwords)\n",
    "    return featureVec\n",
    "\n",
    "\n",
    "def getAvgFeatureVecs(reviews, model, num_features):\n",
    "    # 给定一组评论（每条评论都是一个单词列表），\n",
    "    # 为每条评论计算平均特征向量，并返回一个二维 numpy 数组。\n",
    "    # \n",
    "    # 初始化一个计数器\n",
    "    counter = 0\n",
    "    # \n",
    "    # 为了速度，预先分配一个二维 numpy 数组\n",
    "    reviewFeatureVecs = np.zeros((len(reviews), num_features), dtype=\"float32\")\n",
    "    # \n",
    "    # 遍历所有评论\n",
    "    for review in reviews:\n",
    "       #\n",
    "       # 每处理 5000 条评论，打印一次状态信息\n",
    "       if counter % 5000 == 0:\n",
    "           print (\"正在处理第 %d 条评论，共 %d 条\" % (counter, len(reviews)))\n",
    "       # \n",
    "       # 调用上面定义的函数来生成平均特征向量\n",
    "       reviewFeatureVecs[counter] = makeFeatureVec(review, model, \\\n",
    "           num_features)\n",
    "       #\n",
    "       # 计数器加一\n",
    "       counter = counter + 1\n",
    "    return reviewFeatureVecs\n",
    "# 将影评文本转换为单词列表\n",
    "def review_to_wordlist(review, remove_stopwords=False):\n",
    "    review_text = BeautifulSoup(review, \"lxml\").get_text()\n",
    "    review_text = re.sub(\"[^a-zA-Z]\", \" \", review_text)\n",
    "    words = review_text.lower().split()\n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        words = [w for w in words if not w in stops]\n",
    "    return words\n",
    "\n",
    "# 将影评分割成句子列表，每个句子又是一个单词列表\n",
    "def review_to_sentences(review, tokenizer, remove_stopwords=False):\n",
    "    raw_sentences = tokenizer.tokenize(review.strip())\n",
    "    sentences = []\n",
    "    for raw_sentence in raw_sentences:\n",
    "        if len(raw_sentence) > 0:\n",
    "            sentences.append(review_to_wordlist(raw_sentence, remove_stopwords))\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b43050ad-23fb-49b6-9180-db085ee66c51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在处理第 0 条评论，共 25000 条\n",
      "正在处理第 5000 条评论，共 25000 条\n",
      "正在处理第 10000 条评论，共 25000 条\n",
      "正在处理第 15000 条评论，共 25000 条\n",
      "正在处理第 20000 条评论，共 25000 条\n",
      "正在为测试评论创建平均特征向量\n",
      "正在处理第 0 条评论，共 25000 条\n",
      "正在处理第 5000 条评论，共 25000 条\n",
      "正在处理第 10000 条评论，共 25000 条\n",
      "正在处理第 15000 条评论，共 25000 条\n",
      "正在处理第 20000 条评论，共 25000 条\n"
     ]
    }
   ],
   "source": [
    "# ****************************************************************\n",
    "# 使用我们上面定义的函数，为训练集和测试集计算平均特征向量。\n",
    "# 注意，这次我们进行了停用词移除。\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "train = pd.read_csv(\"../../tutorialData/labeledTrainData.tsv\", header=0, delimiter=\"\\t\", quoting=3)\n",
    "test = pd.read_csv(\"../../tutorialData/testData.tsv\", header=0, delimiter=\"\\t\", quoting=3)\n",
    "num_features = 300    # 词向量维度\n",
    "min_word_count = 40   # 最小词频\n",
    "num_workers = 4       # 并行运行的线程数\n",
    "context = 10          # 上下文窗口大小\n",
    "downsampling = 1e-3   # 高频词的下采样设置\n",
    "clean_train_reviews = []\n",
    "for review in train[\"review\"]:\n",
    "    clean_train_reviews.append( review_to_wordlist( review, \\\n",
    "        remove_stopwords=True ))\n",
    "\n",
    "trainDataVecs = getAvgFeatureVecs( clean_train_reviews, model, num_features )\n",
    "\n",
    "print (\"正在为测试评论创建平均特征向量\")\n",
    "clean_test_reviews = []\n",
    "for review in test[\"review\"]:\n",
    "    clean_test_reviews.append( review_to_wordlist( review, \\\n",
    "        remove_stopwords=True ))\n",
    "\n",
    "testDataVecs = getAvgFeatureVecs( clean_test_reviews, model, num_features )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8eb11b8d-90f9-4fdd-834a-9b0c2dfafeec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在将随机森林模型拟合到带标签的训练数据上...\n",
      "写入完成...\n"
     ]
    }
   ],
   "source": [
    "# 使用100棵决策树，拟合一个随机森林模型\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "forest = RandomForestClassifier( n_estimators = 100 )\n",
    "\n",
    "print (\"正在将随机森林模型拟合到带标签的训练数据上...\")\n",
    "forest = forest.fit( trainDataVecs, train[\"sentiment\"] )\n",
    "\n",
    "# 进行测试并提取结果\n",
    "result = forest.predict( testDataVecs )\n",
    "\n",
    "# 写入测试结果\n",
    "output = pd.DataFrame( data={\"id\":test[\"id\"], \"sentiment\":result} )\n",
    "output.to_csv( \"Word2Vec_AverageVectors.csv\", index=False, quoting=3 )\n",
    "print (\"写入完成...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3263d691-661c-4d41-b7e4-320744c5b318",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KMeans 聚类耗时:  105.08570528030396 秒。\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from gensim.models import Word2Vec\n",
    "import time\n",
    "\n",
    "start = time.time() # 开始时间\n",
    "\n",
    "# 将 \"k\" (聚类数量) 设置为词汇表大小的 1/5，\n",
    "# 即平均每个聚类包含5个单词。\n",
    "model = Word2Vec.load(\"300features_40minwords_10context\")\n",
    "word_vectors = model.wv.vectors \n",
    "num_clusters = word_vectors.shape[0] // 5\n",
    "\n",
    "# 初始化一个KMeans对象，并用它来提取质心\n",
    "# 注意：为了避免未来版本中的 FutureWarning，建议显式设置 n_init='auto'\n",
    "kmeans_clustering = KMeans( n_clusters = num_clusters, n_init='auto' )\n",
    "idx = kmeans_clustering.fit_predict( word_vectors )\n",
    "\n",
    "# 获取结束时间，并打印整个过程所花费的时间\n",
    "end = time.time()\n",
    "elapsed = end - start\n",
    "print (\"KMeans 聚类耗时: \", elapsed, \"秒。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ae5ddf24-9eeb-4604-81dd-1f82026018ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建一个字典，将词汇表中的每个单词映射到其所属的聚类编号\n",
    "word_centroid_map = dict(zip( model.wv.index_to_key, idx ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7681f630-ed19-4fc0-85ae-5d911c27dd1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cluster 0\n",
      "['loyal', 'helpful', 'lifelong']\n",
      "\n",
      "Cluster 1\n",
      "['dangers', 'mythic', 'perils', 'feats', 'superficially', 'secretive']\n",
      "\n",
      "Cluster 2\n",
      "['elliott', 'nicolas', 'gibbs', 'denholm', 'baumbach']\n",
      "\n",
      "Cluster 3\n",
      "['inferno', 'ouch', 'heavenly', 'mercury', 'towering', 'fountain', 'stuntman', 'goliath']\n",
      "\n",
      "Cluster 4\n",
      "['mundane']\n",
      "\n",
      "Cluster 5\n",
      "['lingering', 'zoom', 'microphone']\n",
      "\n",
      "Cluster 6\n",
      "['got', 'saw', 'watched', 'bought', 'rented', 'purchased']\n",
      "\n",
      "Cluster 7\n",
      "['deadly', 'battling', 'havoc', 'deformed', 'backwoods', 'revolt', 'bloodthirsty', 'cannibalistic', 'lurks', 'hordes', 'terrorize', 'horde']\n",
      "\n",
      "Cluster 8\n",
      "['convince', 'teach', 'warn', 'haunt', 'inform', 'embarrass', 'announce', 'informing']\n",
      "\n",
      "Cluster 9\n",
      "['ringo', 'caligula', 'moog']\n"
     ]
    }
   ],
   "source": [
    "# 遍历前10个聚类\n",
    "for cluster in range(0,10):\n",
    "    #\n",
    "    # 打印聚类编号\n",
    "    print (\"\\nCluster %d\" % cluster)\n",
    "    #\n",
    "    # 找出属于该聚类的所有单词，并打印出来\n",
    "    words = []\n",
    "    all_keys = list(word_centroid_map.keys())\n",
    "    all_values = list(word_centroid_map.values())\n",
    "    for i in range(0,len(all_values)):\n",
    "        if( all_values[i] == cluster ):\n",
    "            words.append(all_keys[i])\n",
    "    print (words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fdfeffa-8174-47c5-9674-f02565da6c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bag_of_centroids( wordlist, word_centroid_map ):\n",
    "    #\n",
    "    # 聚类的数量等于单词/质心映射表中的最大聚类索引加一\n",
    "    num_centroids = max( word_centroid_map.values() ) + 1\n",
    "    #\n",
    "    # 预先分配一个质心袋向量（为了提高速度）\n",
    "    bag_of_centroids = np.zeros( num_centroids, dtype=\"float32\" )\n",
    "    #\n",
    "    # 遍历评论中的单词。如果单词在词汇表中，\n",
    "    # 就找到它所属的聚类，并将该聚类的计数加一。\n",
    "    for word in wordlist:\n",
    "        if word in word_centroid_map:\n",
    "            index = word_centroid_map[word]\n",
    "            bag_of_centroids[index] += 1\n",
    "    #\n",
    "    # 返回“质心袋”\n",
    "    return bag_of_centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e5e7b5-c936-40db-8eb4-b87abfcce6da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 预先为训练集分配一个质心袋数组（为了提高速度）\n",
    "train_centroids = np.zeros( (train[\"review\"].size, num_clusters), \\\n",
    "    dtype=\"float32\" )\n",
    "\n",
    "# 将训练集的评论转换为质心袋\n",
    "counter = 0\n",
    "for review in clean_train_reviews:\n",
    "    train_centroids[counter] = create_bag_of_centroids( review, \\\n",
    "        word_centroid_map )\n",
    "    counter += 1\n",
    "\n",
    "# 对测试评论重复同样的操作\n",
    "test_centroids = np.zeros(( test[\"review\"].size, num_clusters), \\\n",
    "    dtype=\"float32\" )\n",
    "\n",
    "counter = 0\n",
    "for review in clean_test_reviews:\n",
    "    test_centroids[counter] = create_bag_of_centroids( review, \\\n",
    "        word_centroid_map )\n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d00ca64c-36c4-4dd1-a6b1-cada494c0f7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在将随机森林模型拟合到带标签的训练数据上...\n",
      "写入完成\n"
     ]
    }
   ],
   "source": [
    "# 拟合随机森林模型并提取预测结果\n",
    "forest = RandomForestClassifier(n_estimators = 100)\n",
    "\n",
    "# 拟合森林模型可能需要几分钟时间\n",
    "print (\"正在将随机森林模型拟合到带标签的训练数据上...\")\n",
    "forest = forest.fit(train_centroids,train[\"sentiment\"])\n",
    "result = forest.predict(test_centroids)\n",
    "\n",
    "# 写入测试结果\n",
    "output = pd.DataFrame(data={\"id\":test[\"id\"], \"sentiment\":result})\n",
    "output.to_csv( \"BagOfCentroids.csv\", index=False, quoting=3 )\n",
    "print (\"写入完成\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd91824-c414-4252-a12b-91c0753d9a07",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
