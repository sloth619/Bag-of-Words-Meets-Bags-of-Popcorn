{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f42a1ba-2cda-4298-a7ee-503fd1eaa270",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 25000 labeled train reviews, 25000 labeled test reviews, and 50000 unlabeled reviews\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 从文件中读取数据\n",
    "train = pd.read_csv( \"../tutorialData/labeledTrainData.tsv\", header=0, \n",
    " delimiter=\"\\t\", quoting=3 )\n",
    "test = pd.read_csv( \"../tutorialData/testData.tsv\", header=0, delimiter=\"\\t\", quoting=3 )\n",
    "unlabeled_train = pd.read_csv( \"../tutorialData/unlabeledTrainData.tsv\", header=0, \n",
    " delimiter=\"\\t\", quoting=3 )\n",
    "\n",
    "# 验证已读取的评论总数（共计 100,000 条）\n",
    "print (\"Read %d labeled train reviews, %d labeled test reviews, \" \\\n",
    " \"and %d unlabeled reviews\\n\" % (train[\"review\"].size,  \n",
    " test[\"review\"].size, unlabeled_train[\"review\"].size ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b66c1b4-7a8d-4857-befc-c5bd630ff753",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def review_to_wordlist( review, remove_stopwords=False ):\n",
    "    # 将文档转换为单词序列的函数，\n",
    "    # 可选择性地移除停用词。返回一个单词列表。\n",
    "    #\n",
    "    # 1. 移除HTML标签\n",
    "    review_text = BeautifulSoup(review).get_text()\n",
    "    #  \n",
    "    # 2. 移除字母以外的字符\n",
    "    review_text = re.sub(\"[^a-zA-Z]\",\" \", review_text)\n",
    "    #\n",
    "    # 3. 将单词转换为小写并进行切分\n",
    "    words = review_text.lower().split()\n",
    "    #\n",
    "    # 4. 可选择性地移除停用词（默认为不移除）\n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        words = [w for w in words if not w in stops]\n",
    "    #\n",
    "    # 5. 返回单词列表\n",
    "    return(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "16ab14b3-abf8-4d13-bc8d-050138f7a0d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    }
   ],
   "source": [
    "# 下载用于分句的 punkt 分词器\n",
    "import nltk.data\n",
    "nltk.download()   \n",
    "\n",
    "# 加载 punkt 分词器\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "# 定义一个函数，将评论拆分为经过解析的句子\n",
    "def review_to_sentences( review, tokenizer, remove_stopwords=False ):\n",
    "    # 该函数将一篇评论拆分成已解析的句子。返回一个\n",
    "    # 句子列表，其中每个句子又是一个单词列表。\n",
    "    #\n",
    "    # 1. 使用 NLTK 的分词器将段落拆分成句子\n",
    "    raw_sentences = tokenizer.tokenize(review.strip())\n",
    "    #\n",
    "    # 2. 遍历每个句子\n",
    "    sentences = []\n",
    "    for raw_sentence in raw_sentences:\n",
    "        # 如果句子为空，则跳过\n",
    "        if len(raw_sentence) > 0:\n",
    "            # 否则，调用 review_to_wordlist 函数获取单词列表\n",
    "            sentences.append( review_to_wordlist( raw_sentence, \\\n",
    "              remove_stopwords ))\n",
    "    #\n",
    "    # 返回句子列表（每个句子是一个单词列表，\n",
    "    # 因此，返回的是一个列表的列表）\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "13d80be5-9c4b-4728-a932-14b489344ae9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在从训练集中解析句子\n",
      "正在从训练集中解析句子\n",
      "正在从无标签数据集中解析句子\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "from bs4 import MarkupResemblesLocatorWarning\n",
    "warnings.filterwarnings(\"ignore\", category=MarkupResemblesLocatorWarning)\n",
    "\n",
    "sentences = []\n",
    "\n",
    "print (\"正在从训练集中解析句子\")\n",
    "sentences = []  \n",
    "\n",
    "print (\"正在从训练集中解析句子\")\n",
    "for review in train[\"review\"]:\n",
    "    sentences += review_to_sentences(review, tokenizer)\n",
    "\n",
    "print (\"正在从无标签数据集中解析句子\")\n",
    "for review in unlabeled_train[\"review\"]:\n",
    "    sentences += review_to_sentences(review, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d8e5b34d-3220-480f-85eb-a847eb360d80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "796172\n"
     ]
    }
   ],
   "source": [
    "print (len(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8d186212-db98-4ebb-8382-27579906d82b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['with', 'all', 'this', 'stuff', 'going', 'down', 'at', 'the', 'moment', 'with', 'mj', 'i', 've', 'started', 'listening', 'to', 'his', 'music', 'watching', 'the', 'odd', 'documentary', 'here', 'and', 'there', 'watched', 'the', 'wiz', 'and', 'watched', 'moonwalker', 'again']\n"
     ]
    }
   ],
   "source": [
    "print (sentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "463af284-8928-4bc2-a6f5-76391a2eeaa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['maybe', 'i', 'just', 'want', 'to', 'get', 'a', 'certain', 'insight', 'into', 'this', 'guy', 'who', 'i', 'thought', 'was', 'really', 'cool', 'in', 'the', 'eighties', 'just', 'to', 'maybe', 'make', 'up', 'my', 'mind', 'whether', 'he', 'is', 'guilty', 'or', 'innocent']\n"
     ]
    }
   ],
   "source": [
    "print (sentences[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7eb5003a-3dc9-4a00-a13a-9dd27088da1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-13 20:47:22,558 : INFO : collecting all words and their counts\n",
      "2025-10-13 20:47:22,560 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2025-10-13 20:47:22,603 : INFO : PROGRESS: at sentence #10000, processed 225664 words, keeping 17775 word types\n",
      "2025-10-13 20:47:22,646 : INFO : PROGRESS: at sentence #20000, processed 451738 words, keeping 24945 word types\n",
      "2025-10-13 20:47:22,688 : INFO : PROGRESS: at sentence #30000, processed 670858 words, keeping 30027 word types\n",
      "2025-10-13 20:47:22,738 : INFO : PROGRESS: at sentence #40000, processed 896840 words, keeping 34335 word types\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在训练模型...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-13 20:47:22,781 : INFO : PROGRESS: at sentence #50000, processed 1116081 words, keeping 37751 word types\n",
      "2025-10-13 20:47:22,824 : INFO : PROGRESS: at sentence #60000, processed 1337543 words, keeping 40711 word types\n",
      "2025-10-13 20:47:22,865 : INFO : PROGRESS: at sentence #70000, processed 1560306 words, keeping 43311 word types\n",
      "2025-10-13 20:47:22,905 : INFO : PROGRESS: at sentence #80000, processed 1779515 words, keeping 45707 word types\n",
      "2025-10-13 20:47:22,942 : INFO : PROGRESS: at sentence #90000, processed 2003713 words, keeping 48121 word types\n",
      "2025-10-13 20:47:22,983 : INFO : PROGRESS: at sentence #100000, processed 2225464 words, keeping 50190 word types\n",
      "2025-10-13 20:47:23,022 : INFO : PROGRESS: at sentence #110000, processed 2444322 words, keeping 52058 word types\n",
      "2025-10-13 20:47:23,062 : INFO : PROGRESS: at sentence #120000, processed 2666487 words, keeping 54098 word types\n",
      "2025-10-13 20:47:23,103 : INFO : PROGRESS: at sentence #130000, processed 2892314 words, keeping 55837 word types\n",
      "2025-10-13 20:47:23,140 : INFO : PROGRESS: at sentence #140000, processed 3104795 words, keeping 57324 word types\n",
      "2025-10-13 20:47:23,177 : INFO : PROGRESS: at sentence #150000, processed 3330431 words, keeping 59045 word types\n",
      "2025-10-13 20:47:23,217 : INFO : PROGRESS: at sentence #160000, processed 3552465 words, keeping 60581 word types\n",
      "2025-10-13 20:47:23,255 : INFO : PROGRESS: at sentence #170000, processed 3776047 words, keeping 62050 word types\n",
      "2025-10-13 20:47:23,288 : INFO : PROGRESS: at sentence #180000, processed 3996236 words, keeping 63483 word types\n",
      "2025-10-13 20:47:23,325 : INFO : PROGRESS: at sentence #190000, processed 4221287 words, keeping 64775 word types\n",
      "2025-10-13 20:47:23,361 : INFO : PROGRESS: at sentence #200000, processed 4445972 words, keeping 66070 word types\n",
      "2025-10-13 20:47:23,403 : INFO : PROGRESS: at sentence #210000, processed 4666510 words, keeping 67367 word types\n",
      "2025-10-13 20:47:23,445 : INFO : PROGRESS: at sentence #220000, processed 4892036 words, keeping 68686 word types\n",
      "2025-10-13 20:47:23,486 : INFO : PROGRESS: at sentence #230000, processed 5113881 words, keeping 69935 word types\n",
      "2025-10-13 20:47:23,523 : INFO : PROGRESS: at sentence #240000, processed 5340847 words, keeping 71144 word types\n",
      "2025-10-13 20:47:23,559 : INFO : PROGRESS: at sentence #250000, processed 5555463 words, keeping 72333 word types\n",
      "2025-10-13 20:47:23,596 : INFO : PROGRESS: at sentence #260000, processed 5775304 words, keeping 73466 word types\n",
      "2025-10-13 20:47:23,635 : INFO : PROGRESS: at sentence #270000, processed 5995572 words, keeping 74740 word types\n",
      "2025-10-13 20:47:23,673 : INFO : PROGRESS: at sentence #280000, processed 6220911 words, keeping 76318 word types\n",
      "2025-10-13 20:47:23,710 : INFO : PROGRESS: at sentence #290000, processed 6443523 words, keeping 77787 word types\n",
      "2025-10-13 20:47:23,749 : INFO : PROGRESS: at sentence #300000, processed 6668258 words, keeping 79142 word types\n",
      "2025-10-13 20:47:23,784 : INFO : PROGRESS: at sentence #310000, processed 6892662 words, keeping 80431 word types\n",
      "2025-10-13 20:47:23,822 : INFO : PROGRESS: at sentence #320000, processed 7118969 words, keeping 81794 word types\n",
      "2025-10-13 20:47:23,855 : INFO : PROGRESS: at sentence #330000, processed 7340486 words, keeping 83006 word types\n",
      "2025-10-13 20:47:23,891 : INFO : PROGRESS: at sentence #340000, processed 7569986 words, keeping 84252 word types\n",
      "2025-10-13 20:47:23,927 : INFO : PROGRESS: at sentence #350000, processed 7792927 words, keeping 85407 word types\n",
      "2025-10-13 20:47:23,965 : INFO : PROGRESS: at sentence #360000, processed 8012526 words, keeping 86567 word types\n",
      "2025-10-13 20:47:24,003 : INFO : PROGRESS: at sentence #370000, processed 8239772 words, keeping 87663 word types\n",
      "2025-10-13 20:47:24,043 : INFO : PROGRESS: at sentence #380000, processed 8465827 words, keeping 88849 word types\n",
      "2025-10-13 20:47:24,081 : INFO : PROGRESS: at sentence #390000, processed 8694607 words, keeping 89883 word types\n",
      "2025-10-13 20:47:24,119 : INFO : PROGRESS: at sentence #400000, processed 8917820 words, keeping 90882 word types\n",
      "2025-10-13 20:47:24,153 : INFO : PROGRESS: at sentence #410000, processed 9138504 words, keeping 91859 word types\n",
      "2025-10-13 20:47:24,188 : INFO : PROGRESS: at sentence #420000, processed 9358474 words, keeping 92880 word types\n",
      "2025-10-13 20:47:24,225 : INFO : PROGRESS: at sentence #430000, processed 9586958 words, keeping 93909 word types\n",
      "2025-10-13 20:47:24,260 : INFO : PROGRESS: at sentence #440000, processed 9812576 words, keeping 94853 word types\n",
      "2025-10-13 20:47:24,296 : INFO : PROGRESS: at sentence #450000, processed 10036719 words, keeping 95995 word types\n",
      "2025-10-13 20:47:24,343 : INFO : PROGRESS: at sentence #460000, processed 10269931 words, keeping 97064 word types\n",
      "2025-10-13 20:47:24,384 : INFO : PROGRESS: at sentence #470000, processed 10496262 words, keeping 97885 word types\n",
      "2025-10-13 20:47:24,421 : INFO : PROGRESS: at sentence #480000, processed 10717170 words, keeping 98809 word types\n",
      "2025-10-13 20:47:24,460 : INFO : PROGRESS: at sentence #490000, processed 10943335 words, keeping 99835 word types\n",
      "2025-10-13 20:47:24,495 : INFO : PROGRESS: at sentence #500000, processed 11165141 words, keeping 100726 word types\n",
      "2025-10-13 20:47:24,534 : INFO : PROGRESS: at sentence #510000, processed 11390498 words, keeping 101672 word types\n",
      "2025-10-13 20:47:24,569 : INFO : PROGRESS: at sentence #520000, processed 11613511 words, keeping 102557 word types\n",
      "2025-10-13 20:47:24,609 : INFO : PROGRESS: at sentence #530000, processed 11838774 words, keeping 103374 word types\n",
      "2025-10-13 20:47:24,648 : INFO : PROGRESS: at sentence #540000, processed 12062185 words, keeping 104231 word types\n",
      "2025-10-13 20:47:24,686 : INFO : PROGRESS: at sentence #550000, processed 12286959 words, keeping 105098 word types\n",
      "2025-10-13 20:47:24,721 : INFO : PROGRESS: at sentence #560000, processed 12509034 words, keeping 105971 word types\n",
      "2025-10-13 20:47:24,758 : INFO : PROGRESS: at sentence #570000, processed 12736827 words, keeping 106757 word types\n",
      "2025-10-13 20:47:24,793 : INFO : PROGRESS: at sentence #580000, processed 12958427 words, keeping 107611 word types\n",
      "2025-10-13 20:47:24,829 : INFO : PROGRESS: at sentence #590000, processed 13184324 words, keeping 108469 word types\n",
      "2025-10-13 20:47:24,864 : INFO : PROGRESS: at sentence #600000, processed 13406550 words, keeping 109190 word types\n",
      "2025-10-13 20:47:24,902 : INFO : PROGRESS: at sentence #610000, processed 13628197 words, keeping 110056 word types\n",
      "2025-10-13 20:47:24,939 : INFO : PROGRESS: at sentence #620000, processed 13852587 words, keeping 110806 word types\n",
      "2025-10-13 20:47:24,975 : INFO : PROGRESS: at sentence #630000, processed 14075900 words, keeping 111574 word types\n",
      "2025-10-13 20:47:25,010 : INFO : PROGRESS: at sentence #640000, processed 14298045 words, keeping 112387 word types\n",
      "2025-10-13 20:47:25,045 : INFO : PROGRESS: at sentence #650000, processed 14522873 words, keeping 113152 word types\n",
      "2025-10-13 20:47:25,082 : INFO : PROGRESS: at sentence #660000, processed 14745444 words, keeping 113891 word types\n",
      "2025-10-13 20:47:25,119 : INFO : PROGRESS: at sentence #670000, processed 14970568 words, keeping 114614 word types\n",
      "2025-10-13 20:47:25,156 : INFO : PROGRESS: at sentence #680000, processed 15194624 words, keeping 115332 word types\n",
      "2025-10-13 20:47:25,192 : INFO : PROGRESS: at sentence #690000, processed 15416772 words, keeping 116100 word types\n",
      "2025-10-13 20:47:25,230 : INFO : PROGRESS: at sentence #700000, processed 15645694 words, keeping 116903 word types\n",
      "2025-10-13 20:47:25,265 : INFO : PROGRESS: at sentence #710000, processed 15865814 words, keeping 117542 word types\n",
      "2025-10-13 20:47:25,305 : INFO : PROGRESS: at sentence #720000, processed 16093341 words, keeping 118184 word types\n",
      "2025-10-13 20:47:25,341 : INFO : PROGRESS: at sentence #730000, processed 16316786 words, keeping 118913 word types\n",
      "2025-10-13 20:47:25,379 : INFO : PROGRESS: at sentence #740000, processed 16539145 words, keeping 119619 word types\n",
      "2025-10-13 20:47:25,416 : INFO : PROGRESS: at sentence #750000, processed 16758550 words, keeping 120265 word types\n",
      "2025-10-13 20:47:25,451 : INFO : PROGRESS: at sentence #760000, processed 16977109 words, keeping 120889 word types\n",
      "2025-10-13 20:47:25,490 : INFO : PROGRESS: at sentence #770000, processed 17203257 words, keeping 121657 word types\n",
      "2025-10-13 20:47:25,529 : INFO : PROGRESS: at sentence #780000, processed 17432842 words, keeping 122359 word types\n",
      "2025-10-13 20:47:25,565 : INFO : PROGRESS: at sentence #790000, processed 17660149 words, keeping 123034 word types\n",
      "2025-10-13 20:47:25,588 : INFO : collected 123505 word types from a corpus of 17798268 raw words and 796172 sentences\n",
      "2025-10-13 20:47:25,589 : INFO : Creating a fresh vocabulary\n",
      "2025-10-13 20:47:25,653 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=40 retains 16490 unique words (13.35% of original 123505, drops 107015)', 'datetime': '2025-10-13T20:47:25.653864', 'gensim': '4.3.3', 'python': '3.10.9 (tags/v3.10.9:1dd9be6, Dec  6 2022, 20:01:21) [MSC v.1934 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.26100-SP0', 'event': 'prepare_vocab'}\n",
      "2025-10-13 20:47:25,654 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=40 leaves 17239122 word corpus (96.86% of original 17798268, drops 559146)', 'datetime': '2025-10-13T20:47:25.654871', 'gensim': '4.3.3', 'python': '3.10.9 (tags/v3.10.9:1dd9be6, Dec  6 2022, 20:01:21) [MSC v.1934 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.26100-SP0', 'event': 'prepare_vocab'}\n",
      "2025-10-13 20:47:25,708 : INFO : deleting the raw counts dictionary of 123505 items\n",
      "2025-10-13 20:47:25,711 : INFO : sample=0.001 downsamples 48 most-common words\n",
      "2025-10-13 20:47:25,712 : INFO : Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 12749795.99803372 word corpus (74.0%% of prior 17239122)', 'datetime': '2025-10-13T20:47:25.712398', 'gensim': '4.3.3', 'python': '3.10.9 (tags/v3.10.9:1dd9be6, Dec  6 2022, 20:01:21) [MSC v.1934 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.26100-SP0', 'event': 'prepare_vocab'}\n",
      "2025-10-13 20:47:25,793 : INFO : estimated required memory for 16490 words and 300 dimensions: 47821000 bytes\n",
      "2025-10-13 20:47:25,794 : INFO : resetting layer weights\n",
      "2025-10-13 20:47:25,813 : INFO : Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2025-10-13T20:47:25.813933', 'gensim': '4.3.3', 'python': '3.10.9 (tags/v3.10.9:1dd9be6, Dec  6 2022, 20:01:21) [MSC v.1934 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.26100-SP0', 'event': 'build_vocab'}\n",
      "2025-10-13 20:47:25,814 : INFO : Word2Vec lifecycle event {'msg': 'training model with 4 workers on 16490 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=10 shrink_windows=True', 'datetime': '2025-10-13T20:47:25.814930', 'gensim': '4.3.3', 'python': '3.10.9 (tags/v3.10.9:1dd9be6, Dec  6 2022, 20:01:21) [MSC v.1934 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.26100-SP0', 'event': 'train'}\n",
      "2025-10-13 20:47:26,824 : INFO : EPOCH 0 - PROGRESS: at 14.05% examples, 1776054 words/s, in_qsize 7, out_qsize 0\n",
      "2025-10-13 20:47:27,831 : INFO : EPOCH 0 - PROGRESS: at 28.48% examples, 1796832 words/s, in_qsize 7, out_qsize 0\n",
      "2025-10-13 20:47:28,834 : INFO : EPOCH 0 - PROGRESS: at 42.19% examples, 1777806 words/s, in_qsize 7, out_qsize 0\n",
      "2025-10-13 20:47:29,838 : INFO : EPOCH 0 - PROGRESS: at 55.27% examples, 1750111 words/s, in_qsize 7, out_qsize 0\n",
      "2025-10-13 20:47:30,847 : INFO : EPOCH 0 - PROGRESS: at 68.88% examples, 1745821 words/s, in_qsize 7, out_qsize 0\n",
      "2025-10-13 20:47:31,853 : INFO : EPOCH 0 - PROGRESS: at 82.83% examples, 1749967 words/s, in_qsize 7, out_qsize 0\n",
      "2025-10-13 20:47:32,858 : INFO : EPOCH 0 - PROGRESS: at 94.65% examples, 1714514 words/s, in_qsize 7, out_qsize 0\n",
      "2025-10-13 20:47:33,275 : INFO : EPOCH 0: training on 17798268 raw words (12751638 effective words) took 7.5s, 1710663 effective words/s\n",
      "2025-10-13 20:47:34,286 : INFO : EPOCH 1 - PROGRESS: at 13.94% examples, 1760810 words/s, in_qsize 7, out_qsize 0\n",
      "2025-10-13 20:47:35,289 : INFO : EPOCH 1 - PROGRESS: at 24.72% examples, 1561837 words/s, in_qsize 7, out_qsize 0\n",
      "2025-10-13 20:47:36,291 : INFO : EPOCH 1 - PROGRESS: at 37.45% examples, 1578051 words/s, in_qsize 7, out_qsize 0\n",
      "2025-10-13 20:47:37,294 : INFO : EPOCH 1 - PROGRESS: at 50.40% examples, 1597402 words/s, in_qsize 7, out_qsize 0\n",
      "2025-10-13 20:47:38,300 : INFO : EPOCH 1 - PROGRESS: at 63.05% examples, 1600410 words/s, in_qsize 7, out_qsize 0\n",
      "2025-10-13 20:47:39,303 : INFO : EPOCH 1 - PROGRESS: at 76.72% examples, 1623948 words/s, in_qsize 7, out_qsize 0\n",
      "2025-10-13 20:47:40,306 : INFO : EPOCH 1 - PROGRESS: at 90.71% examples, 1646379 words/s, in_qsize 7, out_qsize 0\n",
      "2025-10-13 20:47:41,021 : INFO : EPOCH 1: training on 17798268 raw words (12750853 effective words) took 7.7s, 1647674 effective words/s\n",
      "2025-10-13 20:47:42,032 : INFO : EPOCH 2 - PROGRESS: at 12.29% examples, 1551475 words/s, in_qsize 7, out_qsize 0\n",
      "2025-10-13 20:47:43,033 : INFO : EPOCH 2 - PROGRESS: at 24.99% examples, 1580067 words/s, in_qsize 7, out_qsize 0\n",
      "2025-10-13 20:47:44,033 : INFO : EPOCH 2 - PROGRESS: at 38.45% examples, 1622420 words/s, in_qsize 7, out_qsize 0\n",
      "2025-10-13 20:47:45,036 : INFO : EPOCH 2 - PROGRESS: at 52.10% examples, 1651863 words/s, in_qsize 7, out_qsize 0\n",
      "2025-10-13 20:47:46,040 : INFO : EPOCH 2 - PROGRESS: at 65.68% examples, 1669028 words/s, in_qsize 7, out_qsize 0\n",
      "2025-10-13 20:47:47,045 : INFO : EPOCH 2 - PROGRESS: at 79.17% examples, 1676504 words/s, in_qsize 7, out_qsize 0\n",
      "2025-10-13 20:47:48,048 : INFO : EPOCH 2 - PROGRESS: at 92.95% examples, 1687949 words/s, in_qsize 7, out_qsize 0\n",
      "2025-10-13 20:47:48,576 : INFO : EPOCH 2: training on 17798268 raw words (12751170 effective words) took 7.5s, 1689196 effective words/s\n",
      "2025-10-13 20:47:49,582 : INFO : EPOCH 3 - PROGRESS: at 12.39% examples, 1571358 words/s, in_qsize 7, out_qsize 0\n",
      "2025-10-13 20:47:50,585 : INFO : EPOCH 3 - PROGRESS: at 24.56% examples, 1552810 words/s, in_qsize 7, out_qsize 0\n",
      "2025-10-13 20:47:51,585 : INFO : EPOCH 3 - PROGRESS: at 37.95% examples, 1601984 words/s, in_qsize 7, out_qsize 0\n",
      "2025-10-13 20:47:52,587 : INFO : EPOCH 3 - PROGRESS: at 49.99% examples, 1586836 words/s, in_qsize 7, out_qsize 0\n",
      "2025-10-13 20:47:53,588 : INFO : EPOCH 3 - PROGRESS: at 63.45% examples, 1613461 words/s, in_qsize 7, out_qsize 0\n",
      "2025-10-13 20:47:54,590 : INFO : EPOCH 3 - PROGRESS: at 77.50% examples, 1643456 words/s, in_qsize 7, out_qsize 0\n",
      "2025-10-13 20:47:55,591 : INFO : EPOCH 3 - PROGRESS: at 91.55% examples, 1664801 words/s, in_qsize 7, out_qsize 0\n",
      "2025-10-13 20:47:56,195 : INFO : EPOCH 3: training on 17798268 raw words (12748760 effective words) took 7.6s, 1674630 effective words/s\n",
      "2025-10-13 20:47:57,206 : INFO : EPOCH 4 - PROGRESS: at 13.42% examples, 1692024 words/s, in_qsize 7, out_qsize 0\n",
      "2025-10-13 20:47:58,207 : INFO : EPOCH 4 - PROGRESS: at 27.09% examples, 1709924 words/s, in_qsize 7, out_qsize 0\n",
      "2025-10-13 20:47:59,212 : INFO : EPOCH 4 - PROGRESS: at 41.03% examples, 1728211 words/s, in_qsize 7, out_qsize 0\n",
      "2025-10-13 20:48:00,218 : INFO : EPOCH 4 - PROGRESS: at 54.43% examples, 1722752 words/s, in_qsize 7, out_qsize 0\n",
      "2025-10-13 20:48:01,223 : INFO : EPOCH 4 - PROGRESS: at 68.04% examples, 1725465 words/s, in_qsize 7, out_qsize 0\n",
      "2025-10-13 20:48:02,226 : INFO : EPOCH 4 - PROGRESS: at 81.48% examples, 1723048 words/s, in_qsize 7, out_qsize 0\n",
      "2025-10-13 20:48:03,228 : INFO : EPOCH 4 - PROGRESS: at 94.77% examples, 1718629 words/s, in_qsize 7, out_qsize 0\n",
      "2025-10-13 20:48:03,684 : INFO : EPOCH 4: training on 17798268 raw words (12749469 effective words) took 7.5s, 1703582 effective words/s\n",
      "2025-10-13 20:48:03,685 : INFO : Word2Vec lifecycle event {'msg': 'training on 88991340 raw words (63751890 effective words) took 37.9s, 1683447 effective words/s', 'datetime': '2025-10-13T20:48:03.685373', 'gensim': '4.3.3', 'python': '3.10.9 (tags/v3.10.9:1dd9be6, Dec  6 2022, 20:01:21) [MSC v.1934 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.26100-SP0', 'event': 'train'}\n",
      "2025-10-13 20:48:03,686 : INFO : Word2Vec lifecycle event {'params': 'Word2Vec<vocab=16490, vector_size=300, alpha=0.025>', 'datetime': '2025-10-13T20:48:03.686372', 'gensim': '4.3.3', 'python': '3.10.9 (tags/v3.10.9:1dd9be6, Dec  6 2022, 20:01:21) [MSC v.1934 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.26100-SP0', 'event': 'created'}\n",
      "C:\\Users\\sloth\\AppData\\Local\\Temp\\ipykernel_12684\\877368918.py:23: DeprecationWarning: Call to deprecated `init_sims` (Gensim 4.0.0 implemented internal optimizations that make calls to init_sims() unnecessary. init_sims() is now obsoleted and will be completely removed in future versions. See https://github.com/RaRe-Technologies/gensim/wiki/Migrating-from-Gensim-3.x-to-4).\n",
      "  model.init_sims(replace=True)\n",
      "2025-10-13 20:48:03,695 : WARNING : destructive init_sims(replace=True) deprecated & no longer required for space-efficiency\n",
      "2025-10-13 20:48:03,699 : INFO : Word2Vec lifecycle event {'fname_or_handle': '300features_40minwords_10context', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2025-10-13T20:48:03.699923', 'gensim': '4.3.3', 'python': '3.10.9 (tags/v3.10.9:1dd9be6, Dec  6 2022, 20:01:21) [MSC v.1934 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.26100-SP0', 'event': 'saving'}\n",
      "2025-10-13 20:48:03,700 : INFO : not storing attribute cum_table\n",
      "2025-10-13 20:48:03,744 : INFO : saved 300features_40minwords_10context\n"
     ]
    }
   ],
   "source": [
    "# 导入内置的 logging 模块并进行配置，以便 Word2Vec\n",
    "# 输出美观的日志信息\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',\\\n",
    "    level=logging.INFO)\n",
    "\n",
    "# 设置各种参数\n",
    "num_features = 300    # 词向量维度\n",
    "min_word_count = 40   # 最小词频\n",
    "num_workers = 4       # 并行运行的线程数\n",
    "context = 10          # 上下文窗口大小\n",
    "downsampling = 1e-3   # 对高频词进行下采样的设置\n",
    "\n",
    "# 初始化并训练模型（这会需要一些时间）\n",
    "from gensim.models import word2vec\n",
    "print (\"正在训练模型...\")\n",
    "model = word2vec.Word2Vec(sentences, workers=num_workers, \\\n",
    "            vector_size=num_features, min_count = min_word_count, \\\n",
    "            window = context, sample = downsampling)\n",
    "\n",
    "# 如果你后续不再训练该模型，调用\n",
    "# init_sims 会让模型占用更少的内存。\n",
    "model.init_sims(replace=True)\n",
    "\n",
    "# 创建一个有意义的模型名称并保存模型以便将来使用会很有帮助。\n",
    "# 你之后可以使用 Word2Vec.load() 来加载它。\n",
    "model_name = \"300features_40minwords_10context\"\n",
    "model.save(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9e76f0cc-0435-4178-81b1-3f4ccad49bb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'kitchen'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.doesnt_match(\"man woman child kitchen\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6b073030-a3d5-4d99-9df5-118afc1fd8bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'berlin'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.doesnt_match(\"france england germany berlin\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c5f5adb8-6b79-4595-89f9-3deded488752",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'austria'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.doesnt_match(\"paris berlin london austria\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7ff76479-dd78-45a0-a683-657354c315a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('woman', 0.6098932027816772),\n",
       " ('lady', 0.5962560176849365),\n",
       " ('lad', 0.5753629207611084),\n",
       " ('monk', 0.5302724838256836),\n",
       " ('chap', 0.5291829109191895),\n",
       " ('men', 0.5222655534744263),\n",
       " ('millionaire', 0.5186971426010132),\n",
       " ('soldier', 0.5165213942527771),\n",
       " ('guy', 0.5154843330383301),\n",
       " ('farmer', 0.5146099925041199)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(\"man\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "23359b8b-3d48-4a39-ad4c-0c462b8daf22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('princess', 0.673733115196228),\n",
       " ('bride', 0.6021547317504883),\n",
       " ('maid', 0.6014478206634521),\n",
       " ('victoria', 0.5923500061035156),\n",
       " ('prince', 0.5918601751327515),\n",
       " ('mistress', 0.5882285833358765),\n",
       " ('stepmother', 0.5788155198097229),\n",
       " ('showgirl', 0.57710200548172),\n",
       " ('goddess', 0.5735018253326416),\n",
       " ('belle', 0.5659166574478149)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(\"queen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cac3d951-4e08-4577-9363-86f652fe6884",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('terrible', 0.7612918615341187),\n",
       " ('horrible', 0.717065691947937),\n",
       " ('atrocious', 0.7116694450378418),\n",
       " ('abysmal', 0.6962562203407288),\n",
       " ('dreadful', 0.6944637298583984),\n",
       " ('appalling', 0.6726535558700562),\n",
       " ('horrendous', 0.6554230451583862),\n",
       " ('lousy', 0.6256535053253174),\n",
       " ('horrid', 0.6230466365814209),\n",
       " ('laughable', 0.6056016683578491)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(\"awful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8991cb1e-3f77-45f8-89ec-18e7fdfc0c16",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
